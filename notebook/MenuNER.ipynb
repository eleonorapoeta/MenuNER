{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MenuNER"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%cd /content\n",
    "%rm -rf MenuNER\n",
    "!git clone https://github.com/eleonorapoeta/MenuNER.git\n",
    "%cd MenuNER\n",
    "!git checkout origin/develop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing requirements\n",
    "### Changing to the MenuNER directory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "% cd /content/MenuNER"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import torch.cuda\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from ner.preprocess import dataner_preprocess\n",
    "from ner.dataset import MenuDataset\n",
    "from model import BiLSTM_CRF\n",
    "from ner.util_preprocess import pos2ix, convert_examples_to_feature\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from datasets.metric import temp_seed\n",
    "from torch.cuda.amp import GradScaler\n",
    "from ner.util import to_device, evaluation\n",
    "from seqeval.metrics import precision_score, f1_score, recall_score\n",
    "from easydict import EasyDict as edict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Here is not used the Argument parser but all the arguments are inserted in the following dictionary."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "opt = edict({\n",
    "        'data_dir' : '/content/MenuNER/data/menu',\n",
    "        'bert_checkpoint' : '/content/drive/MyDrive/DNLP_Polito/Dataset/Embeddings/bert_plus_eps',\n",
    "        'seed' : 1000,\n",
    "        'train' : 'train.txt',\n",
    "        'max_length' : 512,\n",
    "        'test' : 'test.txt',\n",
    "        'val' : 'valid.txt',\n",
    "        'pos' : True,\n",
    "        'char' : True,\n",
    "        'attention' : True,\n",
    "        'pos_embedding_dim' : 64,\n",
    "        'char_embedding_dim' : 25,\n",
    "        'lr' : 1e-3,\n",
    "        'epochs' : 30,\n",
    "        'eval_and_save_steps' : 300\n",
    "        })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating the Train and Valid Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set seed\n",
    "random.seed(opt.seed)\n",
    "\n",
    "# Processing of data and creation of features - TRAIN/TEST/VAL\n",
    "\n",
    "train_examples = dataner_preprocess(opt.data_dir, opt.train)\n",
    "val_examples = dataner_preprocess(opt.data_dir, opt.val)\n",
    "\n",
    "# Create Feature\n",
    "input_feats_train = convert_examples_to_feature(train_examples)\n",
    "input_feats_val = convert_examples_to_feature(val_examples)\n",
    "\n",
    "# Create train_loader and val_loader\n",
    "\n",
    "dataset_train = MenuDataset(input_feats_train)\n",
    "dataset_val = MenuDataset(input_feats_val)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset_train,\n",
    "                          batch_size=8,\n",
    "                          shuffle=True)\n",
    "\n",
    "valid_loader = DataLoader(dataset=dataset_val,\n",
    "                          batch_size=8,\n",
    "                          shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Main"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "      logger.info(\"%s\", torch.cuda.get_device_name(0))\n",
    "      device = torch.device('cuda')\n",
    "    else:\n",
    "      device = torch.device('cpu')\n",
    "\n",
    "    tag_to_idx = {\"PAD\" : 0, \"MENU\" : 1, \"O\": 2}\n",
    "    with temp_seed(opt.seed):\n",
    "      model = BiLSTM_CRF(tagset_size=len(tag_to_idx),\n",
    "                           bert_checkpoint=opt.bert_checkpoint,\n",
    "                           max_length=opt.max_length,\n",
    "                           embedding_dim=768,\n",
    "                           hidden_dim=512,\n",
    "                           pos2ix=pos2ix(train_examples),\n",
    "                           pos_embedding_dim=opt.pos_embedding_dim,\n",
    "                           pos=opt.pos,\n",
    "                           char=opt.char,\n",
    "                           char_embedding_dim=opt.char_embedding_dim,\n",
    "                           attention=opt.attention).to(device)\n",
    "\n",
    "      # create optimizer, scheduler, scaler, early_stopping\n",
    "      optimizer = AdamW(params=model.parameters(),\n",
    "                        lr=1e-3,\n",
    "                        eps=1e-8,\n",
    "                        weight_decay=0.01)\n",
    "\n",
    "      num_training_steps_per_epoch = len(train_loader)\n",
    "      num_training_steps = num_training_steps_per_epoch * opt.epochs\n",
    "\n",
    "      scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,\n",
    "                                                  num_warmup_steps=0,\n",
    "                                                  num_training_steps=num_training_steps)\n",
    "\n",
    "      scaler = GradScaler()\n",
    "\n",
    "      best_f1_score = - float('inf')\n",
    "      best_f1_score_epoch = 0\n",
    "      path_checkpoint = './train_model_checkpoint.pt'  # modified\n",
    "      for epoch in range(opt.epochs):\n",
    "          train_loss = 0.\n",
    "          local_best_eval_loss = float('inf')\n",
    "          local_best_eval_f1 = 0\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          for step, (x, y) in enumerate(tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch}\")):\n",
    "              model.train()\n",
    "              x = to_device(x, device)\n",
    "              y = to_device(y, device)\n",
    "\n",
    "              mask = torch.sign(torch.abs(x[1])).to(torch.uint8)\n",
    "              logits, predictions = model(x)\n",
    "              log_likelihood = model.crf_module(logits, y, mask=mask, reduction='mean')\n",
    "\n",
    "              loss = log_likelihood * (-1)\n",
    "\n",
    "              # backpropagation\n",
    "              if device == 'cpu':\n",
    "                  loss.backward()\n",
    "              else:\n",
    "                  scaler.scale(loss).backward()\n",
    "\n",
    "              if device == 'cpu':\n",
    "                  optimizer.step()\n",
    "              else:\n",
    "\n",
    "                  scaler.unscale_(optimizer)\n",
    "                  torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                  scaler.step(optimizer)\n",
    "                  scaler.update()\n",
    "\n",
    "              optimizer.zero_grad()\n",
    "              scheduler.step()\n",
    "\n",
    "              print(f\"Epoch {epoch}, local_step: {step}, loss: {loss:.3f}, curr_lr: {scheduler.get_last_lr()[0]:.7f}\")\n",
    "              global_step = (len(train_loader) * epoch) + step\n",
    "\n",
    "              if opt.eval_and_save_steps > 0 and global_step != 0 and global_step % opt.eval_and_save_steps == 0:\n",
    "                  report = evaluation(model, valid_loader, device, epoch)\n",
    "\n",
    "                  if local_best_eval_loss > report['loss']:\n",
    "                      local_best_eval_loss = report['loss']\n",
    "\n",
    "                  if local_best_eval_f1 < report['f1']:\n",
    "                      local_best_eval_f1 = report['f1']\n",
    "\n",
    "                  if report['f1'] > best_f1_score:\n",
    "                      best_f1_score = report['f1']\n",
    "                      with open(path_checkpoint, 'wb') as f:\n",
    "                          checkpoint = model.state_dict()\n",
    "                          torch.save(checkpoint, f)\n",
    "\n",
    "              train_loss += loss.item()\n",
    "\n",
    "          # Evaluate in each epoch\n",
    "          report = evaluation(model, valid_loader, device, epoch)\n",
    "\n",
    "          if local_best_eval_loss > report['loss']:\n",
    "              local_best_eval_loss = report['loss']\n",
    "\n",
    "          if local_best_eval_f1 < report['f1']:\n",
    "              local_best_eval_f1 = report['f1']\n",
    "\n",
    "          if report['f1'] > best_f1_score:\n",
    "              best_f1_score = report['f1']\n",
    "              best_f1_score_epoch = epoch\n",
    "              with open(path_checkpoint, 'wb') as f:\n",
    "                  checkpoint = model.state_dict()\n",
    "                  torch.save(checkpoint, f)\n",
    "\n",
    "          logs = {\n",
    "              'epoch': epoch,\n",
    "              'local_step': step + 1,\n",
    "              'epoch_step': len(train_loader),\n",
    "              'local_best_eval_loss': local_best_eval_loss,\n",
    "              'local_best_eval_f1': local_best_eval_f1,\n",
    "              'best_eval_f1': best_f1_score,\n",
    "          }\n",
    "\n",
    "          logger.info(json.dumps(logs, indent=4))\n",
    "          if (epoch - best_f1_score_epoch) >= 7:  # Our Early stopping\n",
    "              print(f\"Early stopped Training at epoch:{epoch}\")\n",
    "              break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "opt = edict({\n",
    "        'data_dir' : '/content/MenuNER/data/menu',\n",
    "        'test' : 'test.txt',\n",
    "        'pos2ix' : '/content/MenuNER/data/pos2ix_dict.json',\n",
    "        'model_path' : '/content/MenuNER/data/model_checkpoint.pt',\n",
    "        'bert_checkpoint' : '/content/MenuNER/data/bert_checkpoint',\n",
    "        'max_length' : 512,\n",
    "        'pos' : True,\n",
    "        'char' : True,\n",
    "        'attention' : True,\n",
    "        'pos_embedding_dim' : 64,\n",
    "        'char_embedding_dim' : 25\n",
    "      })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Dataset creation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_examples = dataner_preprocess(opt.data_dir, opt.test)\n",
    "input_feats_test = convert_examples_to_feature(test_examples)\n",
    "dataset_test = MenuDataset(input_feats_test)\n",
    "test_loader = DataLoader(dataset=dataset_test,\n",
    "                          batch_size=8,\n",
    "                          shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tag_to_idx = {\"PAD\": 0, \"MENU\": 1, \"O\": 2}\n",
    "\n",
    "def test():\n",
    "\n",
    "    test_examples = dataner_preprocess(opt.data_dir, opt.test)\n",
    "    input_feats_test = convert_examples_to_feature(test_examples)\n",
    "    dataset_val = MenuDataset(input_feats_test)\n",
    "    valid_loader = DataLoader(dataset=dataset_val,\n",
    "                              batch_size=8,\n",
    "                              shuffle=True)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    with open(opt.pos2ix, 'r') as f:\n",
    "        pos2ix = json.load(f)\n",
    "\n",
    "    model = BiLSTM_CRF(tagset_size=len(tag_to_idx),\n",
    "                       bert_checkpoint=opt.bert_checkpoint,\n",
    "                       max_length=opt.max_length,\n",
    "                       embedding_dim=768,\n",
    "                       hidden_dim=512,\n",
    "                       pos2ix=pos2ix,\n",
    "                       pos=opt.pos,\n",
    "                       char=opt.char,\n",
    "                       pos_embedding_dim=opt.pos_embedding_dim,\n",
    "                       char_embedding_dim=opt.char_embedding_dim,\n",
    "                       attention=opt.attention).to(device)\n",
    "\n",
    "    checkpoint = torch.load(opt.model_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    report = evaluation(model, valid_loader, device)\n",
    "\n",
    "    logs = {\n",
    "        'precision': report['precision'],\n",
    "        'f1': report['f1'],\n",
    "        'recall': report['recall']\n",
    "    }\n",
    "\n",
    "    logger.info(json.dumps(logs, indent=4))\n",
    "\n",
    "    print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}